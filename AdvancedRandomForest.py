# -*- coding: utf-8 -*-
"""
@Env: Python2.7
@Time: 2019/10/24 13:31
@Author: zhaoxingfeng
@Function：Random Forest（RF），随机森林二分类
@Version: V1.2
参考文献：
[1] UCI. wine[DB/OL].https://archive.ics.uci.edu/ml/machine-learning-databases/wine.
"""
import pickle

import pandas as pd
import numpy as np
import random
import math
import collections

from imblearn.over_sampling import BorderlineSMOTE
from joblib import Parallel, delayed
from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif


class Tree(object):
    """定义一棵决策树"""

    def __init__(self):
        self.split_feature = None
        self.split_value = None
        self.leaf_value = None
        self.tree_left = None
        self.tree_right = None

    def loadData(self, treeDict):
        if treeDict is None:
            return None
        self.tree_left = None
        self.tree_right = None
        self.split_feature = treeDict['split_feature']
        self.split_value = treeDict['split_value']
        self.leaf_value = treeDict['leaf_value']
        if treeDict['tree_left'] is None:
            self.tree_left = None
        else:
            tempTree = Tree()
            tempTree.loadData(treeDict['tree_left'])
            self.tree_left = tempTree

        if treeDict['tree_right'] is None:
            self.tree_right = None
        else:
            tempTree = Tree()
            tempTree.loadData(treeDict['tree_right'])
            self.tree_right = tempTree

    def calc_predict_value(self, dataset):
        """通过递归决策树找到样本所属叶子节点"""
        if self.leaf_value is not None:
            return self.leaf_value
        elif dataset[self.split_feature] <= self.split_value:
            return self.tree_left.calc_predict_value(dataset)
        else:
            return self.tree_right.calc_predict_value(dataset)

    def describe_tree(self):
        """以json形式打印决策树，方便查看树结构"""
        if not self.tree_left and not self.tree_right:
            leaf_info = "{leaf_value:" + str(self.leaf_value) + "}"
            return leaf_info
        left_info = self.tree_left.describe_tree()
        right_info = self.tree_right.describe_tree()
        tree_structure = "{split_feature:" + str(self.split_feature) + \
                         ",split_value:" + str(self.split_value) + \
                         ",left_tree:" + left_info + \
                         ",right_tree:" + right_info + "}"
        return tree_structure

    def saveTree(self):
        if self is None:
            return None
        treeDict = {}
        treeDict['split_feature'] = self.split_feature
        treeDict['split_value'] = self.split_value
        treeDict['leaf_value'] = self.leaf_value
        if self.tree_left is None:
            treeDict['tree_left'] = self.tree_left
        else:
            treeDict['tree_left'] = self.tree_left.saveTree()

        if self.tree_right is None:
            treeDict['tree_right'] = self.tree_right
        else:
            treeDict['tree_right'] = self.tree_right.saveTree()
        return treeDict


class RandomForestClassifier(object):
    def __init__(self, n_estimators=10, max_depth=-1, min_samples_split=2, min_samples_leaf=1,
                 min_split_gain=0.0, colsample_bytree=None, subsample=0.8, random_state=None, feature_weight=None,
                 base_weight=1):
        """
        随机森林参数
        ----------
        n_estimators:      树数量
        max_depth:         树深度，-1表示不限制深度
        min_samples_split: 节点分裂所需的最小样本数量，小于该值节点终止分裂
        min_samples_leaf:  叶子节点最少样本数量，小于该值叶子被合并
        min_split_gain:    分裂所需的最小增益，小于该值节点终止分裂
        colsample_bytree:  列采样设置，可取[sqrt、log2]。sqrt表示随机选择sqrt(n_features)个特征，
                           log2表示随机选择log(n_features)个特征，设置为其他则不进行列采样
        subsample:         行采样比例
        random_state:      随机种子，设置之后每次生成的n_estimators个样本集不会变，确保实验可重复
        """
        self.n_estimators = n_estimators
        self.max_depth = max_depth if max_depth != -1 else float('inf')
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.min_split_gain = min_split_gain
        self.colsample_bytree = colsample_bytree
        self.subsample = subsample
        self.random_state = random_state
        self.trees = None
        self.feature_weight = feature_weight
        self.base_weight = base_weight

    def fit(self, dataset, targets):
        """模型训练入口"""
        assert targets.unique().__len__() == 2, "There must be two class for targets!"
        targets = targets.to_frame(name='label')

        if self.random_state:
            random.seed(self.random_state)
        # 随机阶段顺序
        random_state_stages = random.sample(range(self.n_estimators), self.n_estimators)

        # 两种列采样方式
        if self.colsample_bytree == "sqrt":
            self.colsample_bytree = int(len(dataset.columns) ** 0.5)
        elif self.colsample_bytree == "log2":
            self.colsample_bytree = int(math.log(len(dataset.columns)))
        else:
            self.colsample_bytree = len(dataset.columns)

        # 并行建立多棵决策树
        self.trees = Parallel(n_jobs=-1, verbose=0, backend="threading")(
            delayed(self._parallel_build_trees)(dataset, targets, random_state)
            for random_state in random_state_stages)

    def _parallel_build_trees(self, dataset, targets, random_state):
        """bootstrap有放回抽样生成训练样本集，建立决策树"""
        # 采样特征的索引
        subcol_index = random.sample(dataset.columns.tolist(), self.colsample_bytree)
        # 样本采样
        # DataFrame.sample
        # n 从轴返回的项目数。不能与压裂一起使用。如果 frac = None，则默认值 = 1。
        # replace允许或禁止对同一行进行多次采样
        # random_state 随机种子
        # reset_index 重置 DataFrame 的索引，并使用默认的索引代替
        # drop 不要尝试将索引插入到数据框列中。这会将索引重置为默认整数索引。
        dataset_stage = dataset.sample(n=int(self.subsample * len(dataset)), replace=True,
                                       random_state=random_state).reset_index(drop=True)
        # 选择特定样本的采样特征
        dataset_stage = dataset_stage.loc[:, subcol_index]
        # 标签采样
        targets_stage = targets.sample(n=int(self.subsample * len(dataset)), replace=True,
                                       random_state=random_state).reset_index(drop=True)

        tree = self._build_single_tree(dataset_stage, targets_stage, depth=0)
        # print(tree.describe_tree())
        return tree

    def _build_single_tree(self, dataset, targets, depth):
        # """递归建立决策树"""
        # print("======================This is a Tree")
        # for item in dataset.columns.values:
        #     print(item)


        # 如果该节点的类别全都一样/样本小于分裂所需最小样本数量，则选取出现次数最多的类别。终止分裂
        if len(targets['label'].unique()) <= 1 or dataset.__len__() <= self.min_samples_split:
            tree = Tree()
            tree.leaf_value = self.calc_leaf_value(targets['label'])
            return tree

        if depth < self.max_depth:
            best_split_feature, best_split_value, best_split_gain = self.choose_best_feature(dataset, targets)
            left_dataset, right_dataset, left_targets, right_targets = \
                self.split_dataset(dataset, targets, best_split_feature, best_split_value)

            tree = Tree()
            # 如果父节点分裂后，左叶子节点/右叶子节点样本小于设置的叶子节点最小样本数量，则该父节点终止分裂
            if left_dataset.__len__() <= self.min_samples_leaf or \
                    right_dataset.__len__() <= self.min_samples_leaf or \
                    best_split_gain <= self.min_split_gain:
                tree.leaf_value = self.calc_leaf_value(targets['label'])
                return tree
            else:

                tree.split_feature = best_split_feature
                tree.split_value = best_split_value
                tree.tree_left = self._build_single_tree(left_dataset, left_targets, depth + 1)
                tree.tree_right = self._build_single_tree(right_dataset, right_targets, depth + 1)
                return tree
        # 如果树的深度超过预设值，则终止分裂
        else:
            tree = Tree()
            tree.leaf_value = self.calc_leaf_value(targets['label'])
            return tree

    def choose_best_feature(self, dataset, targets):
        """寻找最好的数据集划分方式，找到最优分裂特征、分裂阈值、分裂增益"""
        best_split_gain = 1
        best_split_feature = None
        best_split_value = None

        for feature in dataset.columns:
            if dataset[feature].unique().__len__() <= 100:
                unique_values = sorted(dataset[feature].unique().tolist())
            # 如果该维度特征取值太多，则选择100个百分位值作为待选分裂阈值
            else:
                unique_values = np.unique([np.percentile(dataset[feature], x)
                                           for x in np.linspace(0, 100, 100)])

            # 对可能的分裂阈值求分裂增益，选取增益最大的阈值
            for split_value in unique_values:
                left_targets = targets[dataset[feature] <= split_value]
                right_targets = targets[dataset[feature] > split_value]
                split_gain = self.calc_gini(left_targets['label'], right_targets['label'])

                if split_gain < best_split_gain:
                    best_split_feature = feature
                    best_split_value = split_value
                    best_split_gain = split_gain
        # KKKKKKKKK记录选择特征的位置
        return best_split_feature, best_split_value, best_split_gain

    @staticmethod
    def calc_leaf_value(targets):
        """选择样本中出现次数最多的类别作为叶子节点取值"""
        # collections.Counter 是计数器
        label_counts = collections.Counter(targets)
        # zip将内容打包为元组
        major_label = max(zip(label_counts.values(), label_counts.keys()))
        # 返回最大的KEY类型
        return major_label[1]

    @staticmethod
    def calc_gini(left_targets, right_targets):
        """分类树采用基尼指数作为指标来选择最优分裂点"""
        split_gain = 0
        for targets in [left_targets, right_targets]:
            gini = 1
            # 统计每个类别有多少样本，然后计算gini
            label_counts = collections.Counter(targets)
            for key in label_counts:
                prob = label_counts[key] * 1.0 / len(targets)
                gini -= prob ** 2
            split_gain += len(targets) * 1.0 / (len(left_targets) + len(right_targets)) * gini
        return split_gain

    @staticmethod
    def split_dataset(dataset, targets, split_feature, split_value):
        """根据特征和阈值将样本划分成左右两份，左边小于等于阈值，右边大于阈值"""
        left_dataset = dataset[dataset[split_feature] <= split_value]
        left_targets = targets[dataset[split_feature] <= split_value]
        right_dataset = dataset[dataset[split_feature] > split_value]
        right_targets = targets[dataset[split_feature] > split_value]
        return left_dataset, right_dataset, left_targets, right_targets

    def predict(self, dataset):
        """输入样本，预测所属类别"""
        res = []
        for _, row in dataset.iterrows():
            vote_box = {}
            # 统计每棵树的预测结果，选取出现次数最多的结果作为最终类别
            for tree in self.trees:
                # 计算树的权重
                weight = self.getTreeWeightByFeature(tree)
                chose = tree.calc_predict_value(row)
                # print("其中一棵树："+str(chose)+", weight="+str(weight))
                if vote_box.get(chose) != None:
                    vote_box[chose] += weight
                else:
                    vote_box[chose] = weight

            # KKKKKKKKKKKKKKKKK预测方式从一人一票改为统计权重
            pred_label = self.getLabelByMaxWeight(vote_box)
            # print("选择结果："+str(pred_label))
            res.append(pred_label)
        return np.array(res)

    def getTreeWeightByFeature(self, tree):
        if self.feature_weight != None:
            weight = 0
            list_fea = []
            # 遍历数提取分裂特征
            self.getSplitFeaFromTree(tree, list_fea)
            for item in list_fea:
                tmp_weight = self.feature_weight.get(item)
                if tmp_weight != None:
                    weight += math.pow(BASE_WEIGHT, tmp_weight)
                else:
                    print("Unknown Key:"+item)
            return weight
        else:
            return 1

    def getSplitFeaFromTree(self, tree, list_fea):
        if tree.split_feature != None:
            list_fea.append(tree.split_feature)
        if tree.tree_left != None:
            self.getSplitFeaFromTree(tree.tree_left, list_fea)
        if tree.tree_right != None:
            self.getSplitFeaFromTree(tree.tree_right, list_fea)
        return list_fea

    def getLabelByMaxWeight(self, vote_box):
        maxWeight = 0
        maxLabel = "WrongLabel"
        for item in vote_box.keys():
            if maxWeight < vote_box.get(item):
                maxWeight = vote_box.get(item)
                maxLabel = item
        return maxLabel

    def saveModel(self, filePath):
        print("SaveModel")
        dictToSave = {}
        dictToSave['n_estimators'] = self.n_estimators
        dictToSave['max_depth'] = self.max_depth
        dictToSave['min_samples_split'] = self.min_samples_split
        dictToSave['min_samples_leaf'] = self.min_samples_leaf
        dictToSave['min_split_gain'] = self.min_split_gain
        dictToSave['colsample_bytree'] = self.colsample_bytree
        dictToSave['subsample'] = self.subsample
        dictToSave['random_state'] = self.random_state
        dictToSave['feature_weight'] = self.feature_weight
        dictToSave['base_weight'] = self.base_weight

        dictToSave['trees'] = []
        for item in self.trees:
            dictToSave['trees'].append(item.saveTree())

        with open(filePath, 'wb') as f:
            pickle.dump(dictToSave, f)

    def loadModel(self, filePath):
        print("readModel")
        with open(filePath, 'rb') as f:
            dictToSave = pickle.load(f)
        self.n_estimators = dictToSave['n_estimators']
        self.max_depth = dictToSave['max_depth']
        self.min_samples_split = dictToSave['min_samples_split']
        self.min_samples_leaf = dictToSave['min_samples_leaf']
        self.min_split_gain = dictToSave['min_split_gain']
        self.colsample_bytree = dictToSave['colsample_bytree']
        self.subsample = dictToSave['subsample']
        self.random_state = dictToSave['random_state']
        self.feature_weight = dictToSave['feature_weight']
        self.base_weight = dictToSave['base_weight']
        self.trees = []
        for item in dictToSave['trees']:
            tempTree = Tree()
            tempTree.loadData(item)
            self.trees.append(tempTree)

def perf_measure(y_actual, y_hat):
        #     print(y_actual.shape)
        #     print(y_hat.shape)
        TP = 0
        FP = 0
        TN = 0
        FN = 0

        for i in range(len(y_hat)):
            if y_actual[i] == y_hat[i] == 1:
                TP += 1
            if y_hat[i] == 1 and y_actual[i] != y_hat[i]:
                FP += 1
            if y_actual[i] == y_hat[i] == 0:
                TN += 1
            if y_hat[i] == 0 and y_actual[i] != y_hat[i]:
                FN += 1
        if TP > 0:
            presion = TP / (TP + FP)
            recall = TP / (TP + FN)
            myF1 = 2 * presion * recall / (presion + recall)
        else:
            presion = 0
            recall = 0
            myF1 = 0
        return (TP, FP, TN, FN, presion, recall, myF1)


BASE_WEIGHT = 3
if __name__ == '__main__':
    # 自带例子
    # df = pd.read_csv("source/wine.txt")
    # df = df[df['label'].isin([1, 2])].sample(frac=1, random_state=66).reset_index(drop=True)
    # clf = RandomForestClassifier(n_estimators=5,
    #                              max_depth=5,
    #                              min_samples_split=6,
    #                              min_samples_leaf=2,
    #                              min_split_gain=0.0,
    #                              colsample_bytree="sqrt",
    #                              subsample=0.8,
    #                              random_state=66,
    #                              feature_weight={"Malic":2,'Proline':1, 'Magnesium':3})
    # train_count = int(0.7 * len(df))
    # feature_list = ["Alcohol", "Malic acid", "Ash", "Alcalinity of ash", "Magnesium", "Total phenols",
    #                 "Flavanoids", "Nonflavanoid phenols", "Proanthocyanins", "Color intensity", "Hue",
    #                 "OD280/OD315 of diluted wines", "Proline"]
    # clf.fit(df.loc[:train_count, feature_list], df.loc[:train_count, 'label'])
    # clf.saveModel("TestOne")
    # from sklearn import metrics
    # clf1 = RandomForestClassifier()
    # clf1.loadModel("TestOne")
    # print("===========================clf")
    # print(metrics.accuracy_score(df.loc[:train_count, 'label'], clf.predict(df.loc[:train_count, feature_list])))
    # print(metrics.accuracy_score(df.loc[train_count:, 'label'], clf.predict(df.loc[train_count:, feature_list])))
    # print("++++++++++++++++++++++++++++clf")
    # print(metrics.accuracy_score(df.loc[:train_count, 'label'], clf1.predict(df.loc[:train_count, feature_list])))
    # print(metrics.accuracy_score(df.loc[train_count:, 'label'], clf1.predict(df.loc[train_count:, feature_list])))

    # 测试代码2
    # 读取数据文件
    # ！！！！！！！！！！！！开始注释的地方

    # ad2017Path = r'E:\WorkShop\GitlabProjects\droidad\csv\特征调整版\VS2017_AdwareV2_2021_08_10.csv'
    # mal2017Path = r'E:\WorkShop\GitlabProjects\droidad\csv\特征调整版\VS2017_NotAdware_2021_08_13.csv'
    # bnPath = r'E:\WorkShop\GitlabProjects\droidad\csv\特征调整版\BenignV2_2021_08_11.csv'
    # limitSample = 50
    # print("阶段1：读取数据,样本数 :" + str(limitSample * 3))
    # ad2017Data = pd.read_csv(ad2017Path, encoding='cp936')
    # mal2017Data = pd.read_csv(mal2017Path, encoding='cp936')
    # bnData = pd.read_csv(bnPath, encoding='cp936')
    # ad2017Name = ad2017Data.iloc[:, 0]
    # mal2017Name = mal2017Data.iloc[:, 0]
    # bnName = bnData.iloc[:, 0]
    #
    # # 数据采样
    #
    # ad2017Fea = ad2017Data.iloc[:limitSample, 1:]
    # mal2017Fea = mal2017Data.iloc[:limitSample, 1:]
    # bnFea = bnData.iloc[:limitSample, 1:]
    # colNames = ad2017Fea.columns.values
    # # 合并数据
    # X = ad2017Fea.append(mal2017Fea)
    # X = X.append(bnFea)
    # adY = np.ones((len(ad2017Fea), 1), dtype=int)
    # bnY = np.zeros((len(mal2017Fea) + len(bnFea), 1), dtype=int)
    # Y = np.vstack((adY, bnY))
    # Y = np.ravel(Y)
    #
    # # 数据过采样模块
    # # https://blog.csdn.net/u010654299/article/details/103980964
    # from imblearn.over_sampling import BorderlineSMOTE, ADASYN
    # from collections import Counter
    #
    # print("阶段2：过采样数据")
    # print('Original dataset shape %s' % Counter(Y))
    #
    # sm = BorderlineSMOTE(random_state=42, kind="borderline-1")
    # X_bSmote, Y_bSmote = sm.fit_resample(X, Y)
    # print('Resampled dataset shape %s' % Counter(Y_bSmote))
    #
    # ada = ADASYN(random_state=42)
    # X_ada, Y_ada = ada.fit_resample(X, Y)
    # print('Resampled dataset shape %s' % Counter(Y_ada))
    #
    # # 选取特征
    # from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif
    #
    # print("阶段3：特征选择")
    # i = 250
    # skb1_bSmote = SelectKBest(chi2, k=i).fit(X_bSmote, Y_bSmote)
    # skb2_bSmote = SelectKBest(f_classif, k=i).fit(X_bSmote, Y_bSmote)
    # skb3_bSmote = SelectKBest(mutual_info_classif, k=i).fit(X_bSmote, Y_bSmote)
    # list1_bSmote = skb1_bSmote.get_support()
    # list2_bSmote = skb2_bSmote.get_support()
    # list3_bSmote = skb3_bSmote.get_support()
    #
    # i = 250
    # skb1_ada = SelectKBest(chi2, k=i).fit(X_ada, Y_ada)
    # skb2_ada = SelectKBest(f_classif, k=i).fit(X_ada, Y_ada)
    # skb3_ada = SelectKBest(mutual_info_classif, k=i).fit(X_ada, Y_ada)
    # list1_ada = skb1_ada.get_support()
    # list2_ada = skb2_ada.get_support()
    # list3_ada = skb3_ada.get_support()
    #
    #
    # def getSelectIndexList(selected):
    #     selectList = []
    #     for i in range(len(selected)):
    #         if selected[i]:
    #             selectList.append(i)
    #     return selectList
    #
    #
    # # 计算权重bSmote
    # # 不再需要INDEX
    # finalList_bSmote = []
    # finalList_bSmote.extend(getSelectIndexList(list1_bSmote))
    # finalList_bSmote.extend(getSelectIndexList(list2_bSmote))
    # finalList_bSmote.extend(getSelectIndexList(list3_bSmote))
    # finalList_Order_bSmote = list(set(finalList_bSmote))
    # finalList_Order_bSmote.sort()
    #
    # with open("finalList_Order_bSmote", 'w') as f:
    #     tempList = []
    #     for item in finalList_Order_bSmote:
    #         tempList.append(colNames[item])
    #     f.write(",".join(tempList))
    #
    # finalDict_bSmote = {}
    # for item in finalList_bSmote:
    #     if finalDict_bSmote.get(colNames[item]) != None:
    #         finalDict_bSmote[colNames[item]] += 1
    #     else:
    #         finalDict_bSmote[colNames[item]] = 1
    # featureWeightDict_bSmote = finalDict_bSmote
    #
    # # 计算权重ada
    # finalList_ada = []
    # finalList_ada.extend(getSelectIndexList(list1_ada))
    # finalList_ada.extend(getSelectIndexList(list2_ada))
    # finalList_ada.extend(getSelectIndexList(list3_ada))
    # finalList_Order_ada = list(set(finalList_ada))
    # finalList_Order_ada.sort()
    # with open("finalList_Order_ada", 'w') as f:
    #     tempList = []
    #     for item in finalList_Order_ada:
    #         tempList.append(colNames[item])
    #     f.write(",".join(tempList))
    # finalDict_ada = {}
    # for item in finalList_ada:
    #     if finalDict_ada.get(colNames[item]) != None:
    #         finalDict_ada[colNames[item]] += 1
    #     else:
    #         finalDict_ada[colNames[item]] = 1
    #
    # featureWeightDict_ada = finalDict_ada
    #
    # # 设置训练数据集
    # print("阶段4：设置训练数据集")
    # X_bSmote_new = pd.DataFrame(X_bSmote).iloc[:, finalList_Order_bSmote]
    # X_bSmote_new = pd.DataFrame(X_bSmote_new)
    # X_bSmote_new
    #
    # X_ada_new = pd.DataFrame(X_ada).iloc[:, finalList_Order_ada]
    # X_ada_new = pd.DataFrame(X_ada_new)
    # X_ada_new
    #
    #
    # def my_test(sampleType,isSave):
    #     avgTP = []
    #     avgFP = []
    #     avgTN = []
    #     avgFN = []
    #     avgPrecision = []
    #     avgRecall = []
    #     avgF1 = []
    #     if sampleType == 0:
    #         theX = X_bSmote_new
    #         theY = Y_bSmote
    #     else:
    #         theX = X_ada_new
    #         theY = Y_ada
    #
    #     from sklearn.model_selection import StratifiedShuffleSplit
    #     sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=1)
    #
    #     for train_index, test_index in sss.split(theX, theY):
    #         # 训练集用的筛选后的特征，而测试集用完整特征
    #         if sampleType==0:
    #             X_train, X_test = theX.iloc[train_index, :], X_bSmote.iloc[test_index, :]
    #             Y_train, Y_test = theY[train_index], theY[test_index]
    #         else:
    #             X_train, X_test = theX.iloc[train_index, :], X_ada.iloc[test_index, :]
    #             Y_train, Y_test = theY[train_index], theY[test_index]
    #         from AdvancedRandomForest import RandomForestClassifier as MyRF
    #
    #         if isSave:
    #             if sampleType == 0:
    #                 print("+++++++++++++++++++++++++BS Data")
    #                 for item in X_train.columns.values:
    #                     print(item)
    #                 myclf = MyRF(n_estimators=100,
    #                              max_depth=5,
    #                              min_samples_split=6,
    #                              min_samples_leaf=2,
    #                              min_split_gain=0.0,
    #                              colsample_bytree="sqrt",
    #                              subsample=0.8,
    #                              feature_weight=featureWeightDict_bSmote,
    #                              base_weight=3)
    #             else:
    #                 print("+++++++++++++++++++++++++ADA Data")
    #                 for item in X_train.columns.values:
    #                     print(item)
    #                 myclf = MyRF(n_estimators=100,
    #                              max_depth=5,
    #                              min_samples_split=6,
    #                              min_samples_leaf=2,
    #                              min_split_gain=0.0,
    #                              colsample_bytree="sqrt",
    #                              subsample=0.8,
    #                              feature_weight=featureWeightDict_ada,
    #                              base_weight=3)
    #             myclf.fit(pd.DataFrame(X_train), pd.Series(Y_train))
    #             if sampleType==0:
    #                 myclf.saveModel("BorderlineSMOTE_test")
    #             else:
    #                 myclf.saveModel("ADASYN_test")
    #         else:
    #             myclf = MyRF()
    #             if sampleType == 0:
    #                 myclf.loadModel("BorderlineSMOTE_test")
    #             else:
    #                 myclf.loadModel("ADASYN_test")
    #         Y_pred7 = myclf.predict(pd.DataFrame(X_test))
    #         print("我的算法的结果")
    #         rec = perf_measure(Y_test, Y_pred7)
    #         avgTP.append(rec[0])
    #         avgFP.append(rec[1])
    #         avgTN.append(rec[2])
    #         avgFN.append(rec[3])
    #         avgPrecision.append(rec[4])
    #         avgRecall.append(rec[5])
    #         avgF1.append(rec[6])
    #     print("决策树的结果")
    #     if sampleType == 0:
    #         print("数据通过BorderlineSMOTE采样")
    #     else:
    #         print("数据通过ADASYN采样")
    #     print("avgTP = " + str(np.mean(avgTP)))
    #     print("avgFP = " + str(np.mean(avgFP)))
    #     print("avgTN = " + str(np.mean(avgTN)))
    #     print("avgFN = " + str(np.mean(avgFN)))
    #     print("avgPrecision = " + str(np.mean(avgPrecision)))
    #     print("avgRecall = " + str(np.mean(avgRecall)))
    #     print("avgF1 = " + str(np.mean(avgF1)))
    #
    #
    # my_test(0,True)
    # my_test(1,True)
    # my_test(0,False)
    # my_test(1,False)

    # 测试代码3

    import numpy as np
    import pandas
    import matplotlib
    import sklearn
    import csv
    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    from sklearn.datasets import load_iris
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.svm import LinearSVC
    from sklearn.pipeline import make_pipeline
    from sklearn.feature_selection import SelectKBest, f_classif
    from sklearn.svm import LinearSVC
    from sklearn.pipeline import make_pipeline
    from sklearn.preprocessing import StandardScaler
    from sklearn.datasets import make_classification
    from sklearn.feature_selection import SelectKBest, chi2, SelectPercentile, f_classif, mutual_info_classif
    from sklearn.model_selection import KFold
    import json
    from imblearn.over_sampling import BorderlineSMOTE, ADASYN
    from collections import Counter
    import os
    import joblib
    import matplotlib.pyplot as plt

    # 数据保存路径
    modelSaveDir = "新建文件夹\\model"
    dataSaveDir = "新建文件夹\\data"
    def perf_measure(y_actual, y_hat):
        #     print(y_actual.shape)
        #     print(y_hat.shape)
        TP = 0
        FP = 0
        TN = 0
        FN = 0

        for i in range(len(y_hat)):
            if y_actual[i] == y_hat[i] == 1:
                TP += 1
            if y_hat[i] == 1 and y_actual[i] != y_hat[i]:
                FP += 1
            if y_actual[i] == y_hat[i] == 0:
                TN += 1
            if y_hat[i] == 0 and y_actual[i] != y_hat[i]:
                FN += 1
        if TP > 0:
            presion = TP / (TP + FP)
            recall = TP / (TP + FN)
            myF1 = 2 * presion * recall / (presion + recall)
        else:
            presion = 0
            recall = 0
            myF1 = 0
        return (TP, FP, TN, FN, presion, recall, myF1)


    # 读取数据文件
    ad2017Path = r'E:\WorkShop\GitlabProjects\droidad\csv\合并特征\total_ad2017.csv'
    mal2017Path = r'E:\WorkShop\GitlabProjects\droidad\csv\合并特征\total_mal2017.csv'
    bnPath = r'E:\WorkShop\GitlabProjects\droidad\csv\合并特征\total_bn2017.csv'

    ad2017Data = pd.read_csv(ad2017Path, encoding='cp936')
    mal2017Data = pd.read_csv(mal2017Path, encoding='cp936')
    bnData = pd.read_csv(bnPath, encoding='cp936')
    ad2017Name = ad2017Data.iloc[:, 0]
    mal2017Name = mal2017Data.iloc[:, 0]
    bnName = bnData.iloc[:, 0]
    colNames = ad2017Data.iloc[:, 1:].columns.values
    # 数据特征序列

    manifestFeaIdx = list(range(0, 429))
    apiFeaIdx = list(range(429, 12133))
    libFeaIdx = list(range(12133, 12392))
    # 数据采样
    limit = 200
    ad2017Fea = ad2017Data.iloc[:limit, 1:]
    mal2017Fea = mal2017Data.iloc[:limit, 1:]
    bnFea = bnData.iloc[:limit, 1:]
    # 合并数据
    X = ad2017Fea.append(mal2017Fea)
    X = X.append(bnFea)
    adY = np.ones((len(ad2017Fea), 1), dtype=int)
    bnY = np.zeros((len(mal2017Fea) + len(bnFea), 1), dtype=int)
    Y = np.vstack((adY, bnY))
    Y = np.ravel(Y)

    # 数据过采样模块
    # https://blog.csdn.net/u010654299/article/details/103980964

    print('Original dataset shape %s' % Counter(Y))

    sm = BorderlineSMOTE(random_state=42, kind="borderline-1")
    X_bSmote, Y_bSmote = sm.fit_resample(X, Y)
    print('Resampled dataset shape %s' % Counter(Y_bSmote))
    pd.DataFrame(X_bSmote).to_csv(dataSaveDir + os.sep + "X_bSmote.csv", index=False, sep=',')
    pd.DataFrame(Y_bSmote).to_csv(dataSaveDir + os.sep + "Y_bSmote.csv", index=False, sep=',')

    ada = ADASYN(random_state=42)
    X_ada, Y_ada = ada.fit_resample(X, Y)
    print('Resampled dataset shape %s' % Counter(Y_ada))
    pd.DataFrame(X_ada).to_csv(dataSaveDir + os.sep + "X_ada.csv", index=False, sep=',')
    pd.DataFrame(Y_ada).to_csv(dataSaveDir + os.sep + "Y_ada.csv", index=False, sep=',')

    def getSelectIndexList(selected):
        selectList = []
        for i in range(len(selected)):
            if selected[i]:
                selectList.append(i)
        return selectList

    def prework(i, indexList, predix):
        print("选取特征数:" + str(i))
        skb1_bSmote = SelectKBest(chi2, k=i).fit(X_bSmote.iloc[:, indexList], Y_bSmote)
        skb2_bSmote = SelectKBest(f_classif, k=i).fit(X_bSmote.iloc[:, indexList], Y_bSmote)
        skb3_bSmote = SelectKBest(mutual_info_classif, k=i).fit(X_bSmote.iloc[:, indexList], Y_bSmote)
        list1_bSmote = skb1_bSmote.get_support()
        list2_bSmote = skb2_bSmote.get_support()
        list3_bSmote = skb3_bSmote.get_support()

        skb1_ada = SelectKBest(chi2, k=i).fit(X_ada.iloc[:, indexList], Y_ada)
        skb2_ada = SelectKBest(f_classif, k=i).fit(X_ada.iloc[:, indexList], Y_ada)
        skb3_ada = SelectKBest(mutual_info_classif, k=i).fit(X_ada.iloc[:, indexList], Y_ada)
        list1_ada = skb1_ada.get_support()
        list2_ada = skb2_ada.get_support()
        list3_ada = skb3_ada.get_support()

        print("计算权重")

        print("计算权重bSmote")
        finalList_bSmote = []
        finalList_bSmote.extend(getSelectIndexList(list1_bSmote))
        finalList_bSmote.extend(getSelectIndexList(list2_bSmote))
        finalList_bSmote.extend(getSelectIndexList(list3_bSmote))
        finalList_Order_bSmote = list(set(finalList_bSmote))
        finalList_Order_bSmote.sort()
        featureWeightDict_bSmote = {}
        tempCoName = X_bSmote.iloc[:, indexList].columns.values
        for item in finalList_bSmote:
            if featureWeightDict_bSmote.get(tempCoName[item]) != None:
                featureWeightDict_bSmote[tempCoName[item]] += 1
            else:
                featureWeightDict_bSmote[tempCoName[item]] = 1

        print("计算权重ada")
        finalList_ada = []
        finalList_ada.extend(getSelectIndexList(list1_ada))
        finalList_ada.extend(getSelectIndexList(list2_ada))
        finalList_ada.extend(getSelectIndexList(list3_ada))
        finalList_Order_ada = list(set(finalList_ada))
        finalList_Order_ada.sort()
        featureWeightDict_ada = {}
        for item in finalList_ada:
            if featureWeightDict_ada.get(item) != None:
                featureWeightDict_ada[tempCoName[item]] += 1
            else:
                featureWeightDict_ada[tempCoName[item]] = 1

        X_bSmote_new = pd.DataFrame(X_bSmote).iloc[:, finalList_Order_bSmote]
        X_bSmote_new = pd.DataFrame(X_bSmote_new)
        print("设置训练bSmote数据集:" + str(X_bSmote_new.shape))
        X_ada_new = pd.DataFrame(X_ada).iloc[:, finalList_Order_ada]
        X_ada_new = pd.DataFrame(X_ada_new)
        print("设置训练bSmote数据集:" + str(X_ada_new.shape))

        # 保存数据
        # 1 选取的特征名字
        with open(dataSaveDir + os.sep + predix + "_finalList_Order_bSmote_selectedFeatureColumnName_" + str(i),
                  'w') as f:
            tempList = []
            for item in finalList_Order_bSmote:
                tempList.append(tempCoName[item])
            f.write(",".join(tempList))
        with open(dataSaveDir + os.sep + predix + "_finalList_Order_ada_selectedFeatureColumnName_" + str(i), 'w') as f:
            tempList = []
            for item in finalList_Order_ada:
                tempList.append(tempCoName[item])
            f.write(",".join(tempList))

        # 2 特征权重
        with open(dataSaveDir + os.sep + predix + "_featureWeightDict_bSmote_" + str(i), 'w') as f:
            json.dump(featureWeightDict_bSmote, f)
        with open(dataSaveDir + os.sep + predix + "_featureWeightDict_ada_" + str(i), 'w') as f:
            json.dump(featureWeightDict_ada, f)
        # 3 数据集
        X_bSmote_new.to_csv(dataSaveDir + os.sep + predix + "_X_bSmote_new_feaSelected_" + str(i), index=False, sep=',')
        pd.DataFrame(Y_bSmote).to_csv(dataSaveDir + os.sep + predix + "_Y_bSmote_feaSelected_" + str(i), index=False,
                                      sep=',')
        X_ada_new.to_csv(dataSaveDir + os.sep + predix + "_X_ada_new_feaSelected_" + str(i), index=False, sep=',')
        pd.DataFrame(Y_ada).to_csv(dataSaveDir + os.sep + predix + "_Y_ada_feaSelected_" + str(i), index=False, sep=',')

        return X_bSmote_new, Y_bSmote, X_ada_new, Y_ada, featureWeightDict_bSmote, featureWeightDict_ada


    X_bSmote_new_mani, Y_bSmote_mani, X_ada_new_mani, Y_ada_mani, featureWeightDict_bSmote_mani, featureWeightDict_ada_mani = prework(
        100, manifestFeaIdx, "manimanifestFea")
    X_bSmote_new_api, Y_bSmote_api, X_ada_new_api, Y_ada_api, featureWeightDict_bSmote_api, featureWeightDict_ada_api = prework(
        100, apiFeaIdx, "apiFea")
    X_bSmote_new_lib, Y_bSmote_lib, X_ada_new_lib, Y_ada_lib, featureWeightDict_bSmote_lib, featureWeightDict_ada_lib = prework(
        100, libFeaIdx, "libFea")


    def my_test(sampleType, X_bSmote_new, Y_bSmote, X_ada_new, Y_ada, featureWeightDict_bSmote, featureWeightDict_ada,
                fea_num, isSave, predix):
        avgTP = []
        avgFP = []
        avgTN = []
        avgFN = []
        avgPrecision = []
        avgRecall = []
        avgF1 = []
        if sampleType == 0:
            theX = X_bSmote_new
            theY = Y_bSmote
        else:
            theX = X_ada_new
            theY = Y_ada

        from sklearn.model_selection import StratifiedShuffleSplit
        sss = StratifiedShuffleSplit(n_splits=10, test_size=0.3, random_state=1)

        for train_index, test_index in sss.split(theX, theY):
            X_train, X_test = theX.iloc[train_index, :], theX.iloc[test_index, :]
            Y_train, Y_test = theY[train_index], theY[test_index]

            from AdvancedRandomForest import RandomForestClassifier as MyRF
            if isSave:
                if sampleType == 0:
                    myclf = MyRF(n_estimators=100,
                                 max_depth=5,
                                 min_samples_split=6,
                                 min_samples_leaf=2,
                                 min_split_gain=0.0,
                                 colsample_bytree="sqrt",
                                 subsample=0.8,
                                 feature_weight=featureWeightDict_bSmote,
                                 base_weight=3)
                else:
                    myclf = MyRF(n_estimators=100,
                                 max_depth=5,
                                 min_samples_split=6,
                                 min_samples_leaf=2,
                                 min_split_gain=0.0,
                                 colsample_bytree="sqrt",
                                 subsample=0.8,
                                 feature_weight=featureWeightDict_ada,
                                 base_weight=3)
                #     X_test = pd.DataFrame(X_test).rename(columns=testIndex)
                myclf.fit(pd.DataFrame(X_train), pd.Series(Y_train.flat))
                if sampleType == 0:
                    myclf.saveModel(modelSaveDir + os.sep + predix + "my_BS_" + str(fea_num) + ".model")
                else:
                    myclf.saveModel(modelSaveDir + os.sep + predix + "my_ADA_" + str(fea_num) + ".model")
            else:
                myclf = MyRF()
                if sampleType == 0:
                    myclf.loadModel(modelSaveDir + os.sep + predix + "my_BS_" + str(fea_num) + ".model")
                else:
                    myclf.loadModel(modelSaveDir + os.sep + predix + "my_ADA_" + str(fea_num) + ".model")
            Y_pred7 = myclf.predict(pd.DataFrame(X_test))

            rec = perf_measure(Y_test, Y_pred7)
            avgTP.append(rec[0])
            avgFP.append(rec[1])
            avgTN.append(rec[2])
            avgFN.append(rec[3])
            avgPrecision.append(rec[4])
            avgRecall.append(rec[5])
            avgF1.append(rec[6])
        print("决策树的结果")
        if sampleType == 0:
            print("数据通过BorderlineSMOTE采样")
        else:
            print("数据通过ADASYN采样")
        print("我的算法的结果")
        print("avgTP = " + str(np.mean(avgTP)))
        print("avgFP = " + str(np.mean(avgFP)))
        print("avgTN = " + str(np.mean(avgTN)))
        print("avgFN = " + str(np.mean(avgFN)))
        print("avgPrecision = " + str(np.mean(avgPrecision)))
        print("avgRecall = " + str(np.mean(avgRecall)))
        print("avgF1 = " + str(np.mean(avgF1)))
        return (
        np.mean(avgTP), np.mean(avgFP), np.mean(avgTN), np.mean(avgFN), np.mean(avgPrecision), np.mean(avgRecall),
        np.mean(avgF1))


    X_BS = X_bSmote_new_mani.merge(X_bSmote_new_api, left_index=True, right_index=True)



    Y_BS = Y_bSmote_mani
    X_ADA = X_ada_new_mani.merge(X_ada_new_api, left_index=True, right_index=True)
    Y_ADA = Y_ada_mani
    dict_MA_BS = {}
    dict_MA_ADA = {}
    dict_MA_BS.update(featureWeightDict_bSmote_mani)
    dict_MA_BS.update(featureWeightDict_bSmote_api)
    dict_MA_ADA.update(featureWeightDict_ada_mani)
    dict_MA_ADA.update(featureWeightDict_ada_api)
    list_MA_BS = []
    list_MA_ADA = []

    list_MA_BS.append(my_test(0, X_BS, Y_BS, X_ADA, Y_ADA, dict_MA_BS, dict_MA_ADA, 100, True, "MA_"))